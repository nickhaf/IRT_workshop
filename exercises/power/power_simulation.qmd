---
title: "Power Simulation"
subtitle: "Übung"
format: html
---

Eine häufige Frage die bei der Planung von Studien aufkommt, ist die Frage nach der benötigten Stichprobengröße. Eine Möglichkeit diese für eine konkrete Studie zu planen, ist durch Simulationen. Wir haben bereits eine einfache Simulation in der [Übung zu höher parametrisierten Modellen ausprobiert](). In diesem Kapitel wollen wir jetzt darauf aufbauen und uns anschauen, wie Simulationen für das Planen von allen möglichen Studienaspketen genutzt werden können. 

Wir brauchen folgende Pakete: 
```{r}
#| message: false

library(tidyverse)
library(TAM)
library(catIrt)
```

```{r}
#| echo: false

source(here::here("R", "plot_functions.R"))
```

# Das Grundgerüst
Als erstes wollen wir Schritt für Schritt das Grundgerüst für unsere Simulation aufbauen, das wir dann je nach Fragestellung erweitern können. 

## Die Items
Als erstes definieren wir unsere wahren Itemparameter, aus denen dann die Daten simuliert werden.   
Erstelle dafür einen `data.frame`, der pro Item eine Zeile mit den entsprechenden Itemparamtern $\alpha$, $\beta$ und $\gamma$ enthält. 

- Wir wollen 20 Items simulieren.
- $\alpha$ ist der Diskriminationsparameter und soll aus einer Normalverteilung $N(1, 0.1)$ simuliert werden. So ist er noch klein genug, um ein sinnvolles 1PL-Modell ausprobieren zu können. 
- $\beta$ ist der Schwierigkeitsparameter und soll den gesamte erwartete Schwierigkeitsbereich abdecken. Die Range soll von -2 bis 2 gehen, und Itemschwierigkeiten in gleichmäßigen Abständen enthalten. 
- $\gamma$ ist der Rate-Parameter, und wird erst einmal als Null für alle Items angenommen. 

1PL und 2PL Modelle sollten hier also beide gut funktionieren. 

:::{.callout-caution collapse="true"}
## Tipp
Aus einer Normalverteilung können wir mit `rnorm()` samplen.   
Die Schwierigkeitsparamter lassen sich mit `seq()` erzeugen.  
Schaue dir gegebenenfalls die Hilfe an. 
:::

:::{.callout-caution collapse="true"}
## Lösung
```{r}
set.seed(42) ## Setze den gleichen Seed, um exakt die selben Ergebnisse zu bekommen

n_items <- 20 ## So kann ich die Itemzahl im Nachhinein leicht anpassen

item_pars <- data.frame(
  item_id = 1:n_items,
  alpha = rnorm(n_items, 1, 0.1),
  beta = seq(-2, 2, length.out = n_items),
  gamma = rep(0, n_items)
)
```

:::

## Die Antworten

Perfekt, jetzt können wir die Antworten simulieren. Nutze dafür wieder die Funktion `simIRT()` aus `catIrt`. Andere Pakete wie `mirt` haben aber äquivalente Funktionen, und wir haben ja sogar schon [gelernt](), wie man das ganz manuell machen kann. 

Wir müssen neben den Parametern auch die Theta-Werte angeben. Die sollen aus einer Normalverteilung $N(0,1)$ stammen. Hier haben wir natürlich Spielraum: Wenn wir erwarten, dass die Stichprobe, die wir untersuchen wollen, besonders leistungsstark ist, könnten wir auch eine Normalverteilung mit höherem Mittelwert wählen. 

:::{.callout-tip collapse="true"}

## Tipp
Die Parameter müssen als [`Matrix`](https://nickhaf.github.io/IRT_workshop/slides/IntroR/introR.html#/datenstrukturen) dem Funktionsargument `params` übergeben werden. 

:::

:::{.callout-caution collapse="true"}

## Lösung
```{r}
sim_2PL_dat <- simIrt(
  theta = rnorm(100000, 0, 1),
  params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
  mod = "brm"
)
```

:::

## Das Modell
Jetzt können wir das Modell schätzen. Wir geben uns jetzt erst einmal mit einem 1PL-Modell zufrieden. Fitte also ein 1PL-Modell auf die simulierten Daten (z.B. mit `TAM`). 

:::{.callout-tip collapse="true"}
## Tipp
Um an die simulierten Antworten zu kommen, können wir uns einmal die Struktur von dem simulierten Objekt anschauen:

```{r}
str(sim_2PL_dat)
```

Die responses können wir also mit `sim_2PL_dat$resp` extrahieren.

:::


:::{.callout-caution collapse="true"}

## Lösung
```{r}
tam_1PL <- tam(sim_2PL_dat$resp, verbose = FALSE)
```

:::

## Der Vergleich
Jetzt können wir unsere tatsächlichen Schwierigkeitsparameter mit den duch `TAM` geschätzten Schwierigkeitsparamtern vergleichen. Mach das einmal indem du die Differenz bildest. Fällt dir etwas auf?

:::{.callout-note}
Obwohl wir eigentlich ein 2PL Modell simuliert haben (wir haben ja Diskriminationsparamter verschieden von 1 gezogen), haben wir nur ein 1PL Modell gefittet. Das dient einfach dazu zu zeigen, dass die Modellschätzung in Fällen wo die Diskriminationsparamter nicht zu verschieden von 1 sind, hinreichend gut funktionieren kann. 
:::

:::{.callout-tip collapse="true"}
## Tipp

Der Output von `TAM` ist ziemlich groß. Beim Anschauen der Struktur sehen wir nach einigem Suchen, dass die Paramter sich wahrscheinlich in `tam_1PL$item_irt` verstecken. 

:::


:::{.callout-caution collapse="true"}
## Lösung

```{r}
result <- data.frame(
  "b_true" = item_pars$beta,
  "b_est" = tam_1PL$item_irt[, "beta"]
) %>%
  mutate(b_diff = b_est - b_true)

ggplot(data = result, aes(x = b_true, y = abs(b_diff))) +
  geom_point() +
  theme_minimal()
```

Wir können vor allem anhand des Plots gut erkennen, dass die Differenz zwischen den wahren und den geschätzten Werten an den Rändern zunimmt. Besonders Schwere oder besonders leichte Items werden also schlechter geschätzt. Das ist ganz normal, da dies Items relativ wenig Informationen enthalten: Entweder benantworten (fast) alle sie richtig oder (fast) alle sie falsch. Es gibt hier also wenig Variation, die für die Schätzung genutzt werden kann. 

:::

## Wieder von vorn
Sehr gut, jetzt haben wir die einzelnen Bausteine der Simulation zusammengetragen. Jetzt kommt aber die eigentliche Aufgabe: Wir wollen ja verschiedene Stichprobengrößen ausprobieren und anhand der Differenz zwischen den wahren und den geschätzten Parametern beurteilen, welche Stichprobengröße für eine adäquate Schätzung ausreichen könnte. 

### Bedingungen
Zuerst müssen wir die Bedingungen für unsere Simulation festlegen. Erstelle dafür zwei Vektoren. Einer enthält einfach nur die Anzahl an Iterationen für unsere Simulation. Das ist so, als ob wir das Experiment viele Male wiederholen würden, um ein Maß für die Variabilität im Sampling und damit den Parameterschätzungen zu bekommen.  
Der andere sollte ein Vektor aus verschiedenen Stichprobengrößen sein. 

:::{.callout-caution collapse="true"}
## Lösung
```{r}
n_iterations <- 100
n_persons <- seq(100, 500, by = 100)
```
:::

### Schleife
Jetzt brauchen wir einen [genesteten for-Loop]{.highlight}. Der erste for-loop soll den gerade definierten Stichprobengrößen-Vektor entlang gehen. Darin genested brauchen wir einen zweiten for-loop, der von 1 bis zur maximalen, oben definierten Iterations-Anzahl geht. Schaue in den @tip-forloop für mehr Infos zu for-loops. Baue erst einmal nur das Grundgerüst, also die beiden for-loop Definitionen und die (leeren) Klammern. 

:::{#tip-forloop .callout-tip collapse="true"}

```{r}
#| eval: false
```

:::


:::{.callout-caution collapse="true"}

```{r}
```

:::

### Simulation
In diesen for-loop kommt jetzt unsere Simulation. In jeder iteration werden also die Daten neu simuliert. Dabei wird $\theta$ jedesmal neu festgelegt, und zwar nach dem aktuellen Wert, der gerade in der äußeren Schleife durchlaufen wird. Kümmere dich erst einmal nicht darum, dass die Daten abgespeichert werden. Das machen wir im nächsten Schritt. Wir wollen erst einmal nur testen, ob n_persons * n_iterations Antworten simuliert werden. 

:::{.callout-caution collapse="true"}

## Lösung
```{r}
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    sim_2PL_dat <- simIrt(
      theta = rnorm(n, 0, 1),
      params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
      mod = "brm"
    )
  }
}
```

`simIrt()` gibt bei jeder Simulation einen kurzen Text aus. Daran können wir erkennen, dass tatsächlich die gewünschete Anzahl an Datensätzen erzeugt wurde. 
:::

### Modell fitten
Wir könnten jetzt die Daten erst aus der Schleife heraus abspeichern. Da `TAM` aber die Daten im Output enthält, fitten wir einfach direkt das 1PL Modell in der Schleife und speichern die geschätzen Itemschwierigkeiten **im nächsten Schritt** in einem `data.frame` ab, der zudem Eckdaten zur aktuellen Personenanzahl, zur Iteration und zur wahren Itemschwierigkeit enthält.  
Ergänze also die Schleife aus der vorherigen Aufgabe um die `tam()` Funktion, die ein 1PL Modell auf die in der aktuellen Iteration simulierten Daten fittet. Extrahiere aus diesem gefitteten `TAM`-Modell direkt die Itemschwierigkeiten mit `tam_result$item_irt[, "beta"]`.


:::{.callout-caution collapse="true"}

## Lösung
```{r}
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    sim_2PL_dat <- simIrt(
      theta = rnorm(n, 0, 1),
      params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
      mod = "brm"
    )

    tam_result <- tam(sim_2PL_dat$resp, verbose = FALSE)

    beta_est <- tam_result$item_irt[, "beta"]
  }
}
```
Bisher wird das Ergebnis-Objekt immer überschrieben, sodass nur das Ergebnis aus der allerletzten Iteration ausgegeben wird. Das wollen wir ändern!

:::

### Abspeichern
Jetzt müssen wir unsere Ergebnisse noch abspeichern. Erstelle dafür zuerst einen leeren `data.frame`, der die Ergebnisse aufnehmen soll. 

:::{.callout-caution collapse="true"}
```{r}
#| eval: false

results <- data.frame()
for (n in n_persons) {
  for (iter in 1:n_iterations) {
  }
}
```
:::



```{r}
results <- data.frame()
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    sim_2PL_dat <- simIrt(
      theta = rnorm(n, 0, 1),
      params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
      mod = "brm"
    )

    tam_result <- tam(sim_2PL_dat$resp, verbose = FALSE)

    beta_est <- tam_result$item_irt[, "beta"]

    current_result <- data.frame(
      "item" = paste0("Item", 1:n_items),
      "n_persons" = n,
      "iteration" = iter,
      "beta_true" = item_pars$beta,
      "beta_est" = beta_est
    ) %>%
      mutate(beta_diff = beta_est - beta_true)

    results <- rbind(
      results,
      current_result
    )
  }
}


str(results)
```



:::{.callout-caution collapse="true"}

## Lösung
```{r}
#| eval: false

for (n in n_persons) {
  for (iter in 1:n_iterations) {
    sim_2PL_dat <- simIrt(
      theta = rnorm(n, 0, 1),
      params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
      mod = "brm"
    )
  }
}
```


```{r}
#| eval: false


diff_measures <- data.frame("b_est" = itempars_1$beta, "b_true" = items$b) %>%
  mutate(b_diff = b_est - b_true) %>%
  group_by(item, n_persons) ## do the summary for each item and sample size seperatly
summarise(
  MSE = mean(b_diff, na.rm = TRUE), # Calculate the MSE for item difficulties
  MCSE = sd(b_diff, na.rm = TRUE) / sqrt(n() - 1), # Calculate the MCSE
  .groups = "drop"
)

# itempars_2 <- as.data.frame(apply(group_2_2PL$item_irt[, c("alpha", "beta")], 2, round, 2))
# colnames(itempars_2) <- c("alpha_2", "beta_2")

plot(itempars_1$beta, itempars_1$beta)
```


Beispielfrage: What is the appropriate sample size in randomized clinical trials to detect differences in mean scores between groups corresponding to a small treatment effect with adequate power?


- 1 Dimensional, so ähnlich wie bisher, nur mit Schleife drumrum und dann die Güte der Parameterschätzungen beurteilen irgendwie (siehe Morris paper)
- 2 Dimensional:
  - korrelierte Domänen und dann Eindimensional gesondert und einmal mehrdimensional auswerten. Reliabilität ((Konstruktreliabilität, z.B. geteap() in TAM)) sollte steigen, da Informationen vom einen Konstrukt auch bei der Schätzung der Werte auf dem anderen Konstrukt helfen könnten. 
  

Example 1 und 4 sehen interessant aus. 

## 1 Dimensional
Idee: Screening Test für irgendetwas, deswegen ability am unteren Spektrum ansiedeln?


Nach Schroeders 2024:

- Anzahl und Verteilung der Faktoren (eindimensional vs. mehrdimensional)
- Anzahl Itemparamter (z.B. 1PL, also nur Schwierigkeiten für das erste Beispiel) (Anzahl der Items brauchen wir nicht, wir schauen uns einfach Werte über eine bestimmte range an. 
- Item Typ: Dichotom

Testdesign:
(Missings erstmal weglassen?)
- Z.B. zwei Testformen, die gelinkt werden sollen. 
- Missing Pattern (Missing completely at random (MCAR), Linking-Design)
- Wie viel Missing Data

IRT Model und interessierende Paramter
- 1PL, 2PL ...
- software (die wir auch bei der Auswertung nutzen wollen)
- Parameter, die extrahiert werden sollen

Monte Carlo simulation aufsetzen
- Anzahl an iterations:
  - 
- zu evaluierende Sample sizes

  
Evaluation:
- Mean squared error (MSE) der Itemschwierigkeiten wird genommen, um die Präzision der Schätzung zu ermitteln. Acceptable Cutoff below 0.05 (warum?). 

  
The first application example deals with two test forms of a reasoning test that are linked by a subset of common items. The accuracy of item difficulty estimation is examined as a function of sample size.
  
  
https://psych-irt.netlify.app/
