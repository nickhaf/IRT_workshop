---
title: "Stichprobenplanung durch Simulation: for-loop"
subtitle: "Übung"
format: html
bibliography: references.bib
about:
  id: loop-heading
  template: marquee
  image: surf.jpg
---

:::{#loop-heading}

Nachdem wir im vorherigen Schritt das Grundgerüst gebaut haben, wollen wir jetzt die Simulation so erweitern, dass wir verschiedene Stichprobengrößen `n` bei der Datensimulation und Modellschätzung ausprobieren können. 
:::

```{r}
#| echo: false
n_items <- 20
source(here::here("R", "plot_functions.R"))
```

:::{.callout-note}
## Benötigte Pakete
```{r}
#| message: false
library(TAM)
library(tidyverse)
library(catIrt)
```
:::

:::{.callout-caution collapse="true"}
## Vorheriger Code
```{r}
set.seed(42) ## Setze den gleichen Seed, um exakt die selben Ergebnisse zu bekommen

n_items <- 20 ## So kann ich die Itemzahl im Nachhinein leicht anpassen

item_pars <- data.frame(
  item_id = 1:n_items,
  alpha = rnorm(n_items, 1, 0.1),
  beta = seq(-2, 2, length.out = n_items),
  gamma = rep(0, n_items)
)
```
:::

## Wieder von vorn
Sehr gut, wir haben die einzelnen Bausteine der Simulation zusammengetragen. Jetzt kommt aber die eigentliche Aufgabe: Wir wollen ja verschiedene Stichprobengrößen ausprobieren und anhand der Differenz zwischen den wahren und den geschätzten Parametern beurteilen, welche Stichprobengröße für eine adäquate Schätzung ausreichen könnte. 

### Bedingungen
Zuerst müssen wir die Bedingungen für unsere Simulation festlegen. Erstelle dafür zwei Vektoren. Einer enthält einfach nur die Anzahl an Iterationen für unsere Simulation. Das ist so, als ob wir das Experiment viele Male wiederholen würden, um ein Maß für die Variabilität im Sampling und damit den Parameterschätzungen zu bekommen. 
Der andere sollte ein Vektor aus verschiedenen Stichprobengrößen sein.   
<!-- Außerdem sollten wir noch festlegen, welche Abweichung vom Wahren Schwierigkeitswert wir noch akzeptable finden. Wir sagen einfach mal, dass ein mittlerer Unterschied von 0.2 Logits noch in Ordnung ist. D.h. also, wir suchen eine Stichprobenzahl, bei der unsere Abweichung darunter liegt.  -->

:::{.callout-caution collapse="true"}
## Lösung
```{r}
n_iterations <- 100
n_persons <- seq(100, 500, by = 100)
```
:::

### Schleife
Jetzt brauchen wir einen [genesteten for-Loop]{.highlight}. Der erste for-loop soll den gerade definierten Stichprobengrößen-Vektor entlang gehen. Darin genested brauchen wir einen zweiten for-loop, der von 1 bis zur maximalen, oben definierten Iterations-Anzahl geht. Schaue in den @tip-forloop für mehr Infos zu for-loops. Baue erst einmal nur das Grundgerüst, also die beiden for-loop Definitionen und die (leeren) Klammern. 

:::{#tip-forloop .callout-tip collapse="true"}
## Tipp

Generelle Struktur:
```
for(counter in values){
  repeat something with changing counter
}

```

```{r}
#| eval: false
for(i in 10:20){
  new_i <- i * 100
  print(new_i)
}

```

1. In der ersten Iteration nimmt `i` den Wert des ersten Vektorelements nach `in` an, was `10` ist. 
2. Die Operation im Schleifenkörper wird ausgeführt, in diesem Fall wird der aktuelle Wert mit `100` multipliziert und dann mit `print()` in die Konsole ausgegeben. 
3. Nach Abschluss der Operation nimmt `i` den nächsten Wert an, in diesem Fall `11`, und der gesamte Prozess beginnt von neuem. 
4. Die Schleife endet, nachdem die Operation für den letzten Wert ausgeführt wurde, in diesem Fall `20`.

:::


:::{.callout-caution collapse="true"}
## Lösung
```{r}
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    
  }
}
```

By the way: Wir können unseren counter so nennen wie wir wollen, er muss nicht `i` heißen. 
:::

### Simulation
In diesen for-loop kommt jetzt unsere Simulation. In jeder Iteration werden also die Daten neu simuliert. Dabei wird $\theta$ jedesmal neu festgelegt, und zwar nach dem aktuellen Wert, der gerade in der äußeren Schleife durchlaufen wird. Kümmere dich erst einmal nicht darum, dass die Daten abgespeichert werden. Das machen wir im nächsten Schritt. Wir wollen erst einmal nur testen, ob n_persons * n_iterations Antworten simuliert werden. 

:::{.callout-caution collapse="true"}

## Lösung
```{r}
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    sim_2PL_dat <- simIrt(
      theta = rnorm(n, 0, 1),
      params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
      mod = "brm"
    )
  }
}
```

`simIrt()` gibt bei jeder Simulation einen kurzen Text aus. Daran können wir erkennen, dass tatsächlich die gewünschete Anzahl an Datensätzen erzeugt wurde (ok, wir müssten nachzählen, aber das passt erst einmal so). 
:::

### Modell fitten
Wir könnten jetzt die Daten erst aus der Schleife heraus abspeichern. Da `TAM` aber die Daten im Output enthält, fitten wir einfach direkt das 1PL Modell in der Schleife und speichern die geschätzen Itemschwierigkeiten **im nächsten Schritt** in einem `data.frame` ab, der zudem Eckdaten zur aktuellen Personenanzahl, zur Iteration und zur wahren Itemschwierigkeit enthält.  
Ergänze also die Schleife aus der vorherigen Aufgabe um die `tam()` Funktion, die ein 1PL Modell auf die in der aktuellen Iteration simulierten Daten fittet. Extrahiere aus diesem gefitteten `TAM`-Modell direkt die Itemschwierigkeiten mit `tam_result$item_irt[, "beta"]`.


:::{.callout-caution collapse="true"}

## Lösung
```{r}
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    
    # Modell simulieren
    sim_2PL_dat <- simIrt(
      theta = rnorm(n, 0, 1),
      params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
      mod = "brm"
    )

    # Modell fitten
    tam_result <- tam(sim_2PL_dat$resp, verbose = FALSE)

    # Schwierigkeiten extrahieren
    beta_est <- tam_result$item_irt[, "beta"]
  }
}
```
Bisher wird das Ergebnis-Objekt immer überschrieben, sodass nur das Ergebnis aus der allerletzten Iteration ausgegeben wird. Das sollten wir ändern!

:::

### Abspeichern
Jetzt müssen wir unsere Ergebnisse noch abspeichern. Erstelle dafür zuerst einen leeren `data.frame`, der die Ergebnisse aufnehmen soll. 

:::{.callout-caution collapse="true"}
## Lösung
```{r}
#| eval: false

results <- data.frame()
for (n in n_persons) {
  for (iter in 1:n_iterations) {
  }
}
```
:::

Jetzt müssen wir die Ergebnisse aus jedem Durchlauf in einem eigenen `data.frame` festhalten. Erstelle also innerhalb der inneren for-Schleife einen `data.frame`, der folgende Spalten enthält:

- `item`: Die Item-Nummer. Das kann einfach ein Vektor mit den Item aus dem oben erzeugten `data.frame` `item_pars` sein. 
- `n_persons`: Die Anzahl an Personen, die in dieser Iteration simuliert wurden. Die Zahl kann einfach aus dem counter des äußeren for-loops genommen werden. 
- `iteration`: Die aktuelle Iteration für das aktuelle `n`. Kann aus dem counter des inneren loops genommen werden. 
- `beta_true`: Die wahren Itemschwierigkeiten, die wir im `data.frame` `item_pars` festgelegt haben.
- `beta_est`: Die geschätzten Itemschwierigkeiten, die wir im vorherigen Schritt aus dem `TAM`-Modell extrahiert haben.

:::{.callout-caution collapse="true"}
## Lösung
```{r}
#| message: false
#| include: false

results <- data.frame()
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    sim_2PL_dat <- simIrt(
      theta = rnorm(n, 0, 1),
      params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
      mod = "brm"
    )

    tam_result <- tam(sim_2PL_dat$resp, verbose = FALSE)

    beta_est <- tam_result$item_irt[, "beta"]

    current_result <- data.frame(
      "item" = paste0("Item", 1:n_items),
      "n_persons" = n,
      "iteration" = iter,
      "beta_true" = item_pars$beta,
      "beta_est" = beta_est
    ) %>%
      mutate(beta_diff = beta_est - beta_true)

  }
}

```

```{r}
str(results)
```

:::

Zu guter Letzt müssen wir diesen neu erzeugten `data.frame` jetzt noch in jeder Iteration abspeichern. Dafür nutzen wir am besten die Funktion `rbind()`, mit der man `data.frames` untereinander zusammenfügen kann.  
Nutze also innerhalb der Schleife `rbind()`, um den leeren `data.frame` von außerhalb der Schleife mit dem in der Schleife erzeugten Ergebnis-`data.frame` zu verbinden. Das Ergebnis aus dieser Verbindung muss den selben Namen bekommen wie der ursprüngliche, leer `data.frame`, damit das Ergebnis fortlaufend in jeder Iteration erweitert wird.


:::{.callout-caution collapse="true"}
## Lösung
```{r}
#| message: false

results <- data.frame()
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    sim_2PL_dat <- simIrt(
      theta = rnorm(n, 0, 1),
      params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
      mod = "brm"
    )

    tam_result <- tam(sim_2PL_dat$resp, verbose = FALSE)

    beta_est <- tam_result$item_irt[, "beta"]

    current_result <- data.frame(
      "item" = paste0("Item", 1:n_items),
      "n_persons" = n,
      "iteration" = iter,
      "beta_true" = item_pars$beta,
      "beta_est" = beta_est
    )

    results <- rbind(
      results,
      current_result
    )
  }
}


str(results)
```

Das sieht super aus. Der Ergebnis-`data.frame` hat `r nrow(results)` Zeilen, was genau dem Produkt aus Itemzahl, Personenzahl und Iterationszahl entspricht. 

:::

## Auswertung
Jetzt haben wir es gleich geschafft! Wir haben die Daten nach unseren Vorstellungen simuliert und ausprobiert, wie unser Modell damit umgehen kann.  
Jetzt müssen wir nur noch die Ergebnisse auswerten. Dafür nutzen wir den [Mean Squared Error (MSE)]{.highlight}. Das ist einfach der Mittelwert aus der quadrierten Differenz zwischen den wahren und den geschätzten Itemschwierigkeiten. Um den zu berechnen, erstellen wir zuerst eine neue Spalte, die die Differenz zwischen den wahren und den geschätzten Itemschwierigkeiten enthält. Siehe @#tip-mutate zum Erstellen einer neuen Spalte. 

:::{#tip-mutate .callout-tip collapse="true"}

## Tipp

Eine elegante Lösung eine neue Spalte aus der Differenz von zwei Spalten in einem `data.frame` zu bilden ist die `tidyverse`-Funktion `mutate()`: 

```{r}
data.frame(
  "a" = c(1, 2, 3),
  "b" = c(2, 3, 4)
) %>%
  mutate(diff = b - a)
```
:::

:::{.callout-caution collapse="true"}
## Lösung
```{r}
results <- results %>%
  mutate(beta_diff_2 = (beta_est - beta_true)^2)
```
:::

Perfekt. Jetzt müssen wir nur noch den Mittelwert aus diesen Differenzen berechnen. Das ist dann der MSE. Zusätzlich schauen wir uns noch den Monte Carlo Standard Error (MCSE) an, um ein Variabilitätsmaß für den Schätzer MSE zu bekommen. 
Der MCSE wird berechnet, indem wir pro Item pro Personenanzahl die Standardabweichung durch `sqrt(n() - 1)` teilen. Das ist einfach die Wurzel aus der Größe der Gruppe minus 1. 
Das machen wir natürlich nicht über alle Durchläufe hinweg, sondern innerhalb jeder Personenanzahl für jedes Item gesondert. Wir brauchen also pro simulierter Personenzahl pro Item einen Mittelwert (und eine Standardabweichung) über alle Iterationen. Das können wir sehr einfach mit der `group_by()` und `summarize()` Kombination aus dem `tidyverse` machen, siehe @#tip-group. 

:::{#tip-group .callout-tip collapse="true"}
## Tipp

```{r}
data.frame(
  "group_1" = c("a", "a","b", "b","c","c"),
  "c" = c(1, 2, 3, 1, 2, 3)) %>%
  group_by(group_1) %>%
  summarize(
    mean_c = mean(c))

```

Wir gruppieren zuerst nach der Spalte `group_1`. `summarize()` berechnet dann innerhalb jeder Gruppe den Mittelwert aus der Spalte `c` (das sind pro Gruppe 2 Werte in diesem Fall). 
:::

:::{.callout-caution collapse="true"}
## Lösung

```{r}
mse_results <- results %>%
  group_by(n_persons, item) %>%
  summarise(
    MSE = mean(beta_diff_2),
    MCSE = sd(beta_diff_2) / sqrt(n() - 1), 
.groups = "drop"
  )
head(mse_results)
```

`group_by()` gruppiert unsere Daten nach den Spalten `n_persons` und `item`. `summarise()` berechnet dann [innerhalb jeder Gruppenkombination, also pro Personenanzahl pro Item]{.highlight} den Mittelwert und den MCSE. 

:::

## Visualisierung
Klasse. Um die Ergebnisse leichter auszuwerten, können wir sie jetzt noch visualisieren. 

```{r}
mse_results %>%
filter(item %in% c("Item1", "Item10", "Item20")) %>%
ggplot( aes(x = n_persons, y = MSE, colour = item)) +
  geom_point() +
geom_line() +
  geom_ribbon(aes(ymin = MSE - 1.96 * MCSE, ymax = MSE + 1.96 * MCSE, fill = item), 
              alpha = 0.05, color = NA) +
set_colour_scheme() +  
set_fill_scheme() +
theme_bg() +
xlab("Stichprobengröße")

```

Welche Stichprobengröße würdest du für die Studie verwenden?


:::{.callout-caution collapse="true"}
## Lösung

Diese Frage ist natürlich so pauschal nicht beantwortbar. 
Zuerst einmal sehen wir, dass die Daten sich so verhalten, wie wir es erwarten würden, da der MSE mit zunehmender Stichprobengröße abnimmt, der Unterschied zwischen geschätzter und tatsächlicher Itemschwierigkeit wird also kleiner.  
Es fällt außerdem auf, dass die Items 1 und 20 (die wir als sehr leicht und sehr schwer definiert haben) ungenauer geschätzt werden (höherer MSE und auch größeres Konfidenzintervall) als Item 10, das in der Mitte liegt. Das ist auch zu erwarten, da Items, die entweder von fast allen oder von fast niemandem beantwortet werden, weniger Informationen enthalten.

Es ist ein bisschen schwierig, allgemeine Cut-Off Werte für den MSE zu definieren. Ein besseres Vorgehen ist vermutlich, zu schauen, ab welcher Stichprobengröße es keine zu großen Änderungen mehr gibt. Für Item 10 wäre das z.B. in etwa bei 200 Personen. Dort haben wir einen MSE von ca. 0.03, was, wenn wir die Wurzel nehmen, einer Abweichung von ca. 0.17 Logits auf der Itemschwierigkeits-Skala entspricht. Das ist doch ein guter Kompromiss zwischen Genauigkeit und Aufwand von Probanden-Rekrutierung. Wenn wir allerdings an den Rändern interssiert sind, also z.B. vor allem schwere Fragen erstellen wollen, weil wir vor allem die leistungsstarken Testteilnehmer\*innen differenzieren wollen, müssten wir vermutlich eine größere Stichprobe nehmen, damit die schweren Items auch mit einer adäquaten Genauigkeit geschätzt werden können. Gleiches gilt für leicht Items, wenn wir in einer Art Screening die besonders leistungsschwachen Testteilnehmer\*innen herausfiltern wollen. 
::: 


## Ausblick

Dieses Grundgerüst könnten wir jetzt nach unseren Vorstellungen Erweitern. Wir könnten z.B. weitere Domänen hinzufügen und den Zusammenhang mit der Reliabilität anschauen. Wir könnten eine Linkingstudie simulieren, indem wir zwei Gruppen erstellen, von denen jede jeweils einige der Items nicht beantwortet hat. Wir könnten schauen, welche Stichprobengröße nötig wäre, um einen kleinen treatment-Effekt in einem randomisierten klinischen Versuch zu finden.
Sie @#tip-sim für weitere Ressourcen dazu. 
