---
title: "Stichprobenplanung durch Simulation"
subtitle: "Übung"
format: html
bibliographyu: references.bib
---

Eine häufige Frage die bei der Planung von Studien aufkommt, ist die Frage nach der benötigten Stichprobengröße. Eine Möglichkeit diese für eine konkrete Studie zu planen, ist durch Simulationen. Wir haben bereits eine einfache Simulation in der [Übung zu höher parametrisierten Modellen ausprobiert](). In diesem Kapitel wollen wir jetzt darauf aufbauen und uns anschauen, wie Simulationen für das Planen von allen möglichen Studienaspketen genutzt werden können. 

Wir brauchen folgende Pakete: 
```{r}
#| message: false

library(tidyverse)
library(TAM)
library(catIrt)
```

```{r}
#| echo: false

source(here::here("R", "plot_functions.R"))
```

# Das Grundgerüst
Als erstes wollen wir Schritt für Schritt das Grundgerüst für unsere Simulation aufbauen, das wir dann je nach Fragestellung erweitern können. 

## Die Items
Als erstes definieren wir unsere wahren Itemparameter, aus denen dann die Daten simuliert werden.   
Erstelle dafür einen `data.frame`, der pro Item eine Zeile mit den entsprechenden Itemparamtern $\alpha$, $\beta$ und $\gamma$ enthält. 

- Wir wollen 20 Items simulieren.
- $\alpha$ ist der Diskriminationsparameter und soll aus einer Normalverteilung $N(1, 0.1)$ simuliert werden. So ist er noch klein genug, um ein sinnvolles 1PL-Modell ausprobieren zu können. 
- $\beta$ ist der Schwierigkeitsparameter und soll den gesamte erwartete Schwierigkeitsbereich abdecken. Die Range soll von -2 bis 2 gehen, und Itemschwierigkeiten in gleichmäßigen Abständen enthalten. 
- $\gamma$ ist der Rate-Parameter, und wird erst einmal als Null für alle Items angenommen. 

1PL und 2PL Modelle sollten hier also beide gut funktionieren. 

:::{.callout-caution collapse="true"}
## Tipp
Aus einer Normalverteilung können wir mit `rnorm()` samplen.   
Die Schwierigkeitsparamter lassen sich mit `seq()` erzeugen.  
Schaue dir gegebenenfalls die Hilfe an. 
:::

:::{.callout-caution collapse="true"}
## Lösung
```{r}
set.seed(42) ## Setze den gleichen Seed, um exakt die selben Ergebnisse zu bekommen

n_items <- 20 ## So kann ich die Itemzahl im Nachhinein leicht anpassen

item_pars <- data.frame(
  item_id = 1:n_items,
  alpha = rnorm(n_items, 1, 0.1),
  beta = seq(-2, 2, length.out = n_items),
  gamma = rep(0, n_items)
)
```

:::

## Die Antworten

Perfekt, jetzt können wir die Antworten simulieren. Nutze dafür wieder die Funktion `simIRT()` aus `catIrt`. Andere Pakete wie `mirt` haben aber äquivalente Funktionen, und wir haben ja sogar schon [gelernt](), wie man das ganz manuell machen kann. 

Wir müssen neben den Parametern auch die Theta-Werte angeben. Die sollen aus einer Normalverteilung $N(0,1)$ stammen. Hier haben wir natürlich Spielraum: Wenn wir erwarten, dass die Stichprobe, die wir untersuchen wollen, besonders leistungsstark ist, könnten wir auch eine Normalverteilung mit höherem Mittelwert wählen. 

:::{.callout-tip collapse="true"}

## Tipp
Die Parameter müssen als [`Matrix`](https://nickhaf.github.io/IRT_workshop/slides/IntroR/introR.html#/datenstrukturen) dem Funktionsargument `params` übergeben werden. 

:::

:::{.callout-caution collapse="true"}

## Lösung
```{r}
sim_2PL_dat <- simIrt(
  theta = rnorm(100000, 0, 1),
  params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
  mod = "brm"
)
```

:::

## Das Modell
Jetzt können wir das Modell schätzen. Wir geben uns jetzt erst einmal mit einem 1PL-Modell zufrieden. Fitte also ein 1PL-Modell auf die simulierten Daten (z.B. mit `TAM`). 

:::{.callout-tip collapse="true"}
## Tipp
Um an die simulierten Antworten zu kommen, können wir uns einmal die Struktur von dem simulierten Objekt anschauen:

```{r}
str(sim_2PL_dat)
```

Die responses können wir also mit `sim_2PL_dat$resp` extrahieren.

:::


:::{.callout-caution collapse="true"}

## Lösung
```{r}
tam_1PL <- tam(sim_2PL_dat$resp, verbose = FALSE)
```

:::

## Der Vergleich
Jetzt können wir unsere tatsächlichen Schwierigkeitsparameter mit den duch `TAM` geschätzten Schwierigkeitsparamtern vergleichen. Mach das einmal indem du die Differenz bildest. Fällt dir etwas auf?

:::{.callout-note}
Obwohl wir eigentlich ein 2PL Modell simuliert haben (wir haben ja Diskriminationsparamter verschieden von 1 gezogen), haben wir nur ein 1PL Modell gefittet. Das dient einfach dazu zu zeigen, dass die Modellschätzung in Fällen wo die Diskriminationsparamter nicht zu verschieden von 1 sind, hinreichend gut funktionieren kann. 
:::

:::{.callout-tip collapse="true"}
## Tipp

Der Output von `TAM` ist ziemlich groß. Beim Anschauen der Struktur sehen wir nach einigem Suchen, dass die Paramter sich wahrscheinlich in `tam_1PL$item_irt` verstecken. 

:::


:::{.callout-caution collapse="true"}
## Lösung

```{r}
result <- data.frame(
  "b_true" = item_pars$beta,
  "b_est" = tam_1PL$item_irt[, "beta"]
) %>%
  mutate(b_diff = b_est - b_true)

ggplot(data = result, aes(x = b_true, y = abs(b_diff))) +
  geom_point() +
  theme_minimal()
```

Wir können vor allem anhand des Plots gut erkennen, dass die Differenz zwischen den wahren und den geschätzten Werten an den Rändern zunimmt. Besonders Schwere oder besonders leichte Items werden also schlechter geschätzt. Das ist ganz normal, da dies Items relativ wenig Informationen enthalten: Entweder benantworten (fast) alle sie richtig oder (fast) alle sie falsch. Es gibt hier also wenig Variation, die für die Schätzung genutzt werden kann. 

:::

## Wieder von vorn
Sehr gut, jetzt haben wir die einzelnen Bausteine der Simulation zusammengetragen. Jetzt kommt aber die eigentliche Aufgabe: Wir wollen ja verschiedene Stichprobengrößen ausprobieren und anhand der Differenz zwischen den wahren und den geschätzten Parametern beurteilen, welche Stichprobengröße für eine adäquate Schätzung ausreichen könnte. 

### Bedingungen
Zuerst müssen wir die Bedingungen für unsere Simulation festlegen. Erstelle dafür zwei Vektoren. Einer enthält einfach nur die Anzahl an Iterationen für unsere Simulation. Das ist so, als ob wir das Experiment viele Male wiederholen würden, um ein Maß für die Variabilität im Sampling und damit den Parameterschätzungen zu bekommen. 
Der andere sollte ein Vektor aus verschiedenen Stichprobengrößen sein.   
<!-- Außerdem sollten wir noch festlegen, welche Abweichung vom Wahren Schwierigkeitswert wir noch akzeptable finden. Wir sagen einfach mal, dass ein mittlerer Unterschied von 0.2 Logits noch in Ordnung ist. D.h. also, wir suchen eine Stichprobenzahl, bei der unsere Abweichung darunter liegt.  -->

:::{.callout-caution collapse="true"}
## Lösung
```{r}
n_iterations <- 100
n_persons <- seq(100, 500, by = 100)
```
:::

### Schleife
Jetzt brauchen wir einen [genesteten for-Loop]{.highlight}. Der erste for-loop soll den gerade definierten Stichprobengrößen-Vektor entlang gehen. Darin genested brauchen wir einen zweiten for-loop, der von 1 bis zur maximalen, oben definierten Iterations-Anzahl geht. Schaue in den @tip-forloop für mehr Infos zu for-loops. Baue erst einmal nur das Grundgerüst, also die beiden for-loop Definitionen und die (leeren) Klammern. 

:::{#tip-forloop .callout-tip collapse="true"}
```{r}
#| eval: false


```

:::


:::{.callout-caution collapse="true"}

```{r}
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    
  }
}
```

:::

### Simulation
In diesen for-loop kommt jetzt unsere Simulation. In jeder iteration werden also die Daten neu simuliert. Dabei wird $\theta$ jedesmal neu festgelegt, und zwar nach dem aktuellen Wert, der gerade in der äußeren Schleife durchlaufen wird. Kümmere dich erst einmal nicht darum, dass die Daten abgespeichert werden. Das machen wir im nächsten Schritt. Wir wollen erst einmal nur testen, ob n_persons * n_iterations Antworten simuliert werden. 

:::{.callout-caution collapse="true"}

## Lösung
```{r}
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    sim_2PL_dat <- simIrt(
      theta = rnorm(n, 0, 1),
      params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
      mod = "brm"
    )
  }
}
```

`simIrt()` gibt bei jeder Simulation einen kurzen Text aus. Daran können wir erkennen, dass tatsächlich die gewünschete Anzahl an Datensätzen erzeugt wurde. 
:::

### Modell fitten
Wir könnten jetzt die Daten erst aus der Schleife heraus abspeichern. Da `TAM` aber die Daten im Output enthält, fitten wir einfach direkt das 1PL Modell in der Schleife und speichern die geschätzen Itemschwierigkeiten **im nächsten Schritt** in einem `data.frame` ab, der zudem Eckdaten zur aktuellen Personenanzahl, zur Iteration und zur wahren Itemschwierigkeit enthält.  
Ergänze also die Schleife aus der vorherigen Aufgabe um die `tam()` Funktion, die ein 1PL Modell auf die in der aktuellen Iteration simulierten Daten fittet. Extrahiere aus diesem gefitteten `TAM`-Modell direkt die Itemschwierigkeiten mit `tam_result$item_irt[, "beta"]`.


:::{.callout-caution collapse="true"}

## Lösung
```{r}
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    sim_2PL_dat <- simIrt(
      theta = rnorm(n, 0, 1),
      params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
      mod = "brm"
    )

    tam_result <- tam(sim_2PL_dat$resp, verbose = FALSE)

    beta_est <- tam_result$item_irt[, "beta"]
  }
}
```
Bisher wird das Ergebnis-Objekt immer überschrieben, sodass nur das Ergebnis aus der allerletzten Iteration ausgegeben wird. Das wollen wir ändern!

:::

### Abspeichern
Jetzt müssen wir unsere Ergebnisse noch abspeichern. Erstelle dafür zuerst einen leeren `data.frame`, der die Ergebnisse aufnehmen soll. 

:::{.callout-caution collapse="true"}
## Lösung
```{r}
#| eval: false

results <- data.frame()
for (n in n_persons) {
  for (iter in 1:n_iterations) {
  }
}
```
:::

Jetzt müssen wir die Ergebnisse aus jedem Durchlauf in einem eigenen `data.frame` festhalten. Erstelle also innerhalb der inneren for-Schleife einen `data.frame`, der folgende Spalten enthält:

- `item`: Die Item-Nummer. Das kann einfach ein Vektor mit den Item aus dem oben erzeugten `data.frame` item_pars` sein. 
- `n_persons`: Die Anzahl an Personen, die in dieser Iteration simuliert wurden. Die Zahl kann einfach aus dem Zähler des äußeren for-loops genommen werden. 
- `iteration`: Die aktuelle Iteration für das aktuelle `n`. Kann aus dem Zähler des inneren loops genommen werden. 
- `beta_true`: Die wahren Itemschwierigkeiten, die wir im `data.frame` `item_pars` festgelegt haben.
- `beta_est`: Die geschätzten Itemschwierigkeiten, die wir im vorherigen Schritt aus dem `TAM`-Modell extrahiert haben.

:::{.callout-caution collapse="true"}
## Lösung
```{r}
#| message: false
#| include: false

results <- data.frame()
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    sim_2PL_dat <- simIrt(
      theta = rnorm(n, 0, 1),
      params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
      mod = "brm"
    )

    tam_result <- tam(sim_2PL_dat$resp, verbose = FALSE)

    beta_est <- tam_result$item_irt[, "beta"]

    current_result <- data.frame(
      "item" = paste0("Item", 1:n_items),
      "n_persons" = n,
      "iteration" = iter,
      "beta_true" = item_pars$beta,
      "beta_est" = beta_est
    ) %>%
      mutate(beta_diff = beta_est - beta_true)

  }
}

```

```{r}
str(results)
```

:::

Zu guter Letzt müssen wir diesen neu erzeugten `data.frame` jetzt noch in jeder Iteration abspeichern. Dafür nutzen wir am besten die Funktion `rbind()`, mit der man `data.frames` untereinander zusammenfügen kann.  
Nutze also innerhalb der Schleife `rbind()`, um den leeren `data.frame` von außerhalb der Schleife mit dem in der Schleife erzeugten Ergebnis-`data.frame` zu verbinden. Das Ergebnis aus dieser Verbindung muss den selben Namen bekommen wie der ursprüngliche, leer `data.frame`, damit das Ergebnis fortlaufend in jeder Iteration erweitert wird.


:::{.callout-caution collapse="true"}
## Lösung
```{r}
#| message: false

results <- data.frame()
for (n in n_persons) {
  for (iter in 1:n_iterations) {
    sim_2PL_dat <- simIrt(
      theta = rnorm(n, 0, 1),
      params = as.matrix(item_pars[, c("alpha", "beta", "gamma")]),
      mod = "brm"
    )

    tam_result <- tam(sim_2PL_dat$resp, verbose = FALSE)

    beta_est <- tam_result$item_irt[, "beta"]

    current_result <- data.frame(
      "item" = paste0("Item", 1:n_items),
      "n_persons" = n,
      "iteration" = iter,
      "beta_true" = item_pars$beta,
      "beta_est" = beta_est
    )

    results <- rbind(
      results,
      current_result
    )
  }
}


str(results)
```

Das sieht super aus. Der Ergebnis-`data.frame` hat `r nrow(results)` Zeilen, was genau dem Produkt aus Itemzahl, Personenzahl und Iterationszahl entspricht. 

:::

## Auswertung
Jetzt haben wir es gleich geschafft! Wir haben die Daten nach unseren Vorstellungen simuliert und ausprobiert, wie unser Modell damit umgehen kann.  
Jetzt müssen wir nur noch die Ergebnisse auswerten. Dafür nutzen wir den [Mean Squared Error (MSE)]{.highlight}. Das ist einfach der Mittelwert aus der quadrierten Differenz zwischen den wahren und den geschätzten Itemschwierigkeiten. Um den zu berechnen, erstellen wir zuerst eine neue Spalte, die die Differenz zwischen den wahren und den geschätzten Itemschwierigkeiten enthält. Siehe @#tip-mutate zum Erstellen einer neuen Spalte. 

:::{#tip-mutate .callout-tip collapse = "true"}
## Tipp

Eine elegante Lösung eine neue Spalte aus der Differenz von zwei Spalten in einem `data.frame` zu bilden ist die `tidyverse`-Funktion `mutate()`: 

```{r}
data.frame(
  "a" = c(1, 2, 3),
  "b" = c(2, 3, 4)
) %>%
  mutate(diff = b - a)
```
:::

:::{.callout-caution collapse="true"}
## Lösung
```{r}
results <- results %>%
  mutate(beta_diff_2 = (beta_est - beta_true)^2)
```
:::

Perfekt. Jetzt müssen wir nur noch den Mittelwert aus diesen Differenzen berechnen. Das ist dann der MSE. Zusätzlich schauen wir uns noch den Monte Carlo Standard Error (MCSE) an, um ein Variabilitätsmaß für den Schätzer MSE zu bekommen. 
Der MCSE wird berechnet, indem wir pro Item pro Personenanzahl die Standardabweichung durch `sqrt(n() - 1)` teilen. Das ist einfach die Wurzel aus der Größe der Gruppe minus 1. 
Das machen wir natürlich nicht über alle Durchläufe hinweg, sondern innerhalb jeder Personenanzahl für jedes Item gesondert. Wir brauchen also pro simulierter Personenzahl pro Item einen Mittelwert (und eine Standardabweichung) über alle Iterationen. Das können wir sehr einfach mit der `group_by()` und `summarize()` Kombination aus dem `tidyverse` machen, siehe @#tip-group. 

:::{#tip-group .callout-tip collapse = "true"}
## Tipp

```{r}
data.frame(
  "group_1" = c("a", "a","b", "b","c","c"),
  "c" = c(1, 2, 3, 1, 2, 3)) %>%
  group_by(group_1) %>%
  summarize(
    mean_c = mean(c))

```

Wir gruppieren zuerst nach der Spalte `group_1`. `summarize()` berechnet dann innerhalb jeder Gruppe den Mittelwert aus der Spalte `c` (das sind pro Gruppe 2 Werte in diesem Fall). 
:::

:::{.callout-caution collapse = "true"}
## Lösung

```{r}
mse_results <- results %>%
  group_by(n_persons, item) %>%
  summarise(
    MSE = mean(beta_diff_2),
    MCSE = sd(beta_diff_2) / sqrt(n() - 1), 
.groups = "drop"
  ) %>%
  mutate(RMSE = sqrt(MSE), 
         RMCSE = sqrt(MCSE)) ## Ich nehme hier noch die Wurzel aus dem MSE, damit die Ergebnisse wieder auf der Schwierigkeitsskala sind. 

head(mse_results)
```

`group_by()` gruppiert unsere Daten nach den Spalten `n_persons` und `item`. `summarise()` berechnet dann [innerhalb jeder Gruppenkombination, also pro Personenanzahl pro Item]{.highlight} den Mittelwert und den MCSE. 

:::

## Visualisierung
Klasse. Um die Ergebnisse leichter auszuwerten, können wir sie jetzt noch visualisieren. 

```{r}
mse_results %>%
filter(item %in% c("Item1", "Item10", "Item20")) %>%
ggplot( aes(x = n_persons, y = RMSE, colour = item)) +
  geom_point() +
geom_line() +
  geom_ribbon(aes(ymin = RMSE - 1.96 * MCSE, ymax = RMSE + 1.96 * MCSE, fill = item), 
              alpha = 0.05, color = NA) +
  geom_hline(yintercept = 0.2, linetype = "dashed", colour = '#01364C') + ## Cutoff Wert)
## and some styling, not relevant, but the functions can be found in the R folder on GitHub
set_colour_scheme() +  
set_fill_scheme() +
theme_bg() +
xlab("Stichprobengröße")

```

Fantastisch. Die Daten verhalten sich so, wie wir es erwarten würden, da der MSE mit zunehmender Stichprobengröße abnimmt, der Unterschied zwischen geschätzter und tatsächlicher Itemschwierigkeit wird also kleiner. 


Wir haben ja oben unseren Cutoff Wert berechnet, und können daran erkennen, dass wir für unser kleines Modell eine Stichprobengröße von ... bräuchten, um die Itemschwierigkeiten präzise genug zu schätzen. 

Und Cutoff oben festlegen! 

:::{.callout-note}
Diese Grundgerüst können wir jetzt natürlich entsprechend unserer Fragestellung erweitern. Einige mögliche Fragestellungen:


Für mehr Infos, und weiteren (auch komplexeren) [Codebeispielen](https://psych-irt.netlify.app/), siehe @schroederssample.  
In @morris2019using findet man eine tolle Übersicht zum Workflow in Simulationsstudien allgemein. 

:::


## Ausblick
Dieses Grundgerüst könnten wir jetzt nach unseren Vorstellungen Erweitern. Wir könnten z.B. weitere Domänen hinzufügen und den Zusammenhang mit der Reliabilität anschauen. Wir könnten eine Linkingstudie simulieren, indem wir zwei Gruppen erstellen, von denen jede jeweils einige der Items nicht beantwortet hat. Wir könnten schauen, welche Stichprobengröße nötig wäre, um einen kleinen treatment-Effekt in einem randomisierten klinischen Versuch zu finden.

